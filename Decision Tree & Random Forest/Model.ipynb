{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing the necessary libraries\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import xlogy\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import graphviz\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "except ModuleNotFoundError:\n",
    "    %pip install graphviz\n",
    "    from graphviz import Digraph\n",
    "\n",
    "# Remove all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for the Model\n",
    "\"\"\"\n",
    "\n",
    "def entropy(Y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Y: pd.Series: Output values\n",
    "\n",
    "    Returns: float: Entropy\n",
    "    \"\"\"\n",
    "\n",
    "    vals = Y.value_counts(normalize=True)\n",
    "    return -np.sum(xlogy(vals, vals))\n",
    "\n",
    "def gini_index(Y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Y: pd.Series: Output values\n",
    "\n",
    "    Returns: float: Gini Index\n",
    "    \"\"\"\n",
    "\n",
    "    vals = Y.value_counts(normalize=True)\n",
    "    return 1 - np.sum(np.square(vals))\n",
    "\n",
    "def information_gain(parent: pd.Series, left: pd.Series, right: pd.Series):\n",
    "    \"\"\"\n",
    "    parent: pd.Series: Input parent dataset.\n",
    "    left: pd.Series: Subset of the parent dataset.\n",
    "    right: pd.Series: Subset of the parent dataset.\n",
    "\n",
    "    Returns: float: Information gain.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate parent and child entropy\n",
    "    before_entropy = entropy(parent)\n",
    "    after_entropy = (len(left) / len(parent)) * entropy(left) + (len(right) / len(parent)) * entropy(right)\n",
    "        \n",
    "    # calculate information gain \n",
    "    information_gain = before_entropy - after_entropy\n",
    "    return information_gain\n",
    "\n",
    "def best_split(dataset: pd.DataFrame, num_samples: int, num_features: int):\n",
    "    \"\"\"\n",
    "    dataset: pd.DataFrame: The dataset to split.\n",
    "    num_samples: int: The number of samples in the dataset.\n",
    "    num_features: int: The number of features in the dataset.\n",
    "\n",
    "    Returns: dict: A dictionary with the best split.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Find the best split\n",
    "    best_split = {'gain': -1, 'feature': None, 'threshold': None, \"left_dataset\": None, \"right_dataset\": None}\n",
    "    for feature_index in range(num_features):\n",
    "        feature_values = dataset.iloc[:, feature_index]\n",
    "        thresholds = np.unique(feature_values)\n",
    "        for threshold in thresholds:\n",
    "            left_dataset, right_dataset = split_data(dataset, feature_index, threshold)\n",
    "            y, left_y, right_y = dataset.iloc[:, -1], left_dataset.iloc[:, -1], right_dataset.iloc[:, -1]\n",
    "            gain = information_gain(y, left_y, right_y)\n",
    "            if gain > best_split[\"gain\"]:\n",
    "                best_split[\"gain\"] = gain\n",
    "                best_split[\"feature\"] = feature_index\n",
    "                best_split[\"threshold\"] = threshold\n",
    "                best_split[\"left_dataset\"] = left_dataset\n",
    "                best_split[\"right_dataset\"] = right_dataset\n",
    "    return best_split\n",
    "\n",
    "def split_data(dataset: pd.DataFrame, feature: int, threshold: float):\n",
    "    \"\"\"\n",
    "    dataset: pd.DataFrame: Input dataset.\n",
    "    feature: int: Index of the feature to be split on.\n",
    "    threshold: float: Threshold value to split the feature on.\n",
    "\n",
    "    Returns:\n",
    "        left_dataset: pd.DataFrame: Subset of the dataset.\n",
    "        right_dataset: pd.DataFrame: Subset of the dataset.\n",
    "    \"\"\"\n",
    "    # Create mask of the dataset using threshold\n",
    "    mask = (dataset.iloc[:, feature] <= threshold)\n",
    "\n",
    "    # Mask the dataset\n",
    "    left_dataset = dataset[mask]\n",
    "    right_dataset = dataset[~mask]\n",
    "    return left_dataset, right_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "Size of the dataset:  (150, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert to DataFrame\n",
    "dataset = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "dataset['target'] = iris.target\n",
    "\n",
    "# Print the first few records\n",
    "print(dataset.head())\n",
    "\n",
    "# Print the size of the dataset\n",
    "print(\"Size of the dataset: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Train Dataset:\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "22           0.775771          0.607125           0.168646          0.033729\n",
      "15           0.773811          0.597328           0.203635          0.054303\n",
      "65           0.769454          0.356016           0.505313          0.160782\n",
      "11           0.786991          0.557452           0.262330          0.032791\n",
      "42           0.786090          0.571702           0.232254          0.035731\n",
      "size: (120, 4)\n",
      "\n",
      "Target Train Dataset:\n",
      "22    0\n",
      "15    0\n",
      "65    1\n",
      "11    0\n",
      "42    0\n",
      "Name: target, dtype: int32\n",
      "size: (120,)\n",
      "\n",
      "Feature Test Dataset:\n",
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "73            0.736599          0.338111           0.567543          0.144905\n",
      "18            0.806828          0.537885           0.240633          0.042465\n",
      "118           0.706006          0.238392           0.632655          0.210885\n",
      "78            0.733509          0.354530           0.550132          0.183377\n",
      "76            0.764673          0.314865           0.539769          0.157433\n",
      "size: (30, 4)\n",
      "\n",
      "Target Test Dataset:\n",
      "73     1\n",
      "18     0\n",
      "118    2\n",
      "78     1\n",
      "76     1\n",
      "Name: target, dtype: int32\n",
      "size: (30,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract X and Y from dataset\n",
    "X = dataset.iloc[:, :-1]\n",
    "Y = dataset.iloc[:, -1]\n",
    "\n",
    "# Normalize the dataset\n",
    "X = pd.DataFrame(preprocessing.normalize(X), columns=X.columns)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Print the first few records\n",
    "print(f'Feature Train Dataset:\\n{X_train.head()}\\nsize: {X_train.shape}\\n')\n",
    "print(f'Target Train Dataset:\\n{Y_train.head()}\\nsize: {Y_train.shape}\\n')\n",
    "print(f'Feature Test Dataset:\\n{X_test.head()}\\nsize: {X_test.shape}\\n')\n",
    "print(f'Target Test Dataset:\\n{Y_test.head()}\\nsize: {Y_test.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"\n",
    "    A class representing a node in a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        \"\"\"\n",
    "        feature: string: The feature used for splitting at this node.\n",
    "        threshold: List of float: The threshold used for splitting at this node.\n",
    "        left: Node: Pointer to the left Node.\n",
    "        Right: Node: Pointer to the Right Node.\n",
    "        gain: float: The gain of the split.\n",
    "        value: float: predicted value at this node.\n",
    "        \"\"\"\n",
    "\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    A decision tree classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor for DecisionTree class.\n",
    "\n",
    "        min_samples: int: Minimum number of samples at leaf node.\n",
    "        max_depth: int: Maximum depth of the decision tree.\n",
    "        \"\"\"\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def build_tree(self, dataset: pd.DataFrame, current_depth=0):\n",
    "        \"\"\"\n",
    "        dataset: pd.DataFrame: The dataset to build the tree.\n",
    "        current_depth: int: The current depth of the tree.\n",
    "\n",
    "        Returns: Node: The root node of the decision tree.\n",
    "        \"\"\"\n",
    "        \n",
    "        # split the dataset into X, y values\n",
    "        X, y = dataset.iloc[:, :-1], dataset.iloc[:, -1]\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Terminating conditions\n",
    "        if n_samples >= self.min_samples and current_depth <= self.max_depth:\n",
    "            best_split_values = best_split(dataset, n_samples, n_features)\n",
    "            left_node = self.build_tree(best_split_values[\"left_dataset\"], current_depth + 1)\n",
    "            right_node = self.build_tree(best_split_values[\"right_dataset\"], current_depth + 1)\n",
    "\n",
    "            return Node(best_split_values[\"feature\"], best_split_values[\"threshold\"], left_node, right_node, best_split_values[\"gain\"])\n",
    "\n",
    "        # compute leaf node value\n",
    "        leaf_value = y.mean()\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        X: pd.DataFrame: The feature datset.\n",
    "        y: pd.Series: The target values.\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset = pd.concat([X, y], axis=1) \n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        X: pd.DataFrame: The feature matrix to make predictions for.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of predicted class labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = X.apply(self.traverse_tree, axis=1, args=(self.root,))\n",
    "        return predictions\n",
    "    \n",
    "    def traverse_tree(self, X: pd.Series, node: Node):\n",
    "        \"\"\"\n",
    "        X: pd.Series: The feature vector to predict the target value for.\n",
    "        node: Node: The current node being evaluated.\n",
    "\n",
    "        Returns: float: The predicted target value.\n",
    "        \"\"\"\n",
    "        \n",
    "        if node.value != None: # if the node is a leaf node\n",
    "            return node.value\n",
    "        else: # if the node is not a leaf node\n",
    "            feature = X.iloc[node.feature]\n",
    "            if feature <= node.threshold:\n",
    "                return self.traverse_tree(X, node.left)\n",
    "            else:\n",
    "                return self.traverse_tree(X, node.right)\n",
    "            \n",
    "    def plot_tree(self, node=None, depth=0, dot=None):\n",
    "        \"\"\"\n",
    "        Plot the decision tree.\n",
    "        \"\"\"\n",
    "        if dot is None:\n",
    "            dot = Digraph()\n",
    "            dot.attr('node', shape='box')\n",
    "            dot.node(name=str(node), label=str(node.value))\n",
    "        else:\n",
    "            if node is not None:\n",
    "                dot.node(name=str(node) + str(depth), label=str(node.value))\n",
    "                if node.left is not None:\n",
    "                    dot.edge(str(node) + str(depth), str(node.left) + str(depth+1))\n",
    "                    self.plot_tree(node.left, depth+1, dot)\n",
    "                if node.right is not None:\n",
    "                    dot.edge(str(node) + str(depth), str(node.right) + str(depth+1))\n",
    "                    self.plot_tree(node.right, depth+1, dot)\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model for Decision Tree\n",
    "model = DecisionTree(min_samples=2, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.02321428571428571\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Calculating the metrics\n",
    "Y_pred = model.predict(X_train)\n",
    "print(f\"Train MSE: {mean_squared_error(Y_train, Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.15884353741496596\n"
     ]
    }
   ],
   "source": [
    "# Predicting the values\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating the metrics\n",
    "print(f'Test MSE: {mean_squared_error(Y_test, Y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
