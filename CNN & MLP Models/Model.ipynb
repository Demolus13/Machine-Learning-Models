{"cells":[{"cell_type":"markdown","metadata":{"id":"3GzAcuFnVjyK"},"source":["## Multi-Layer Perceptron Models"]},{"cell_type":"markdown","metadata":{"id":"P1UamQJvVjyM"},"source":["### Imports and Utils"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4389,"status":"ok","timestamp":1717559745019,"user":{"displayName":"Parth Govale","userId":"05489880960182395172"},"user_tz":-330},"id":"ERcTLuzKVjyM","outputId":"d429cbcc-a496-4b03-faed-fa877c9828f3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}],"source":["\"\"\"\n","Importing the necessary libraries\n","\"\"\"\n","import re\n","import os\n","from time import time\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","from torchtext.data.utils import get_tokenizer\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import numpy as np\n","import string\n","\n","# Remove all the warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set env CUDA_LAUNCH_BLOCKING=1\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717559745022,"user":{"displayName":"Parth Govale","userId":"05489880960182395172"},"user_tz":-330},"id":"btXTZnfxVjyN"},"outputs":[],"source":["# Function to load the data\n","def load_text_data(file_path):\n","    \"\"\"\n","    file_path: str: The path to the file\n","\n","    Returns: str: The text data in the file\n","    \"\"\"\n","    # Load the data\n","    with open(file_path, 'rb') as file:\n","        data = file.read().decode('utf-8')\n","\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"fnPKqVs9VjyN"},"source":["### Dataset Loading and Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HRfA6ttVjyO"},"outputs":[],"source":["# Load the data\n","file_path = '/content/corpora/Start-ups.txt'\n","file_data = load_text_data(file_path)\n","\n","# Initialize the tokenizer\n","tokenizer = get_tokenizer('basic_english')\n","tokens = tokenizer(file_data)\n","\n","# Get the unique tokens\n","words = sorted(list(set(tokens)))\n","words.append('<unk>')\n","\n","# Create encoding and decoding dictionaries\n","encodings = {token: idx for idx, token in enumerate(words)}\n","decodings = {idx: token for token, idx in encodings.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3d_T5s7VjyO"},"outputs":[],"source":["block_size = 4\n","\n","# Create the dataset with the encoding\n","dataset = [encodings[token] for token in tokens]\n","\n","# Create the input and target sequences\n","input_seq = [dataset[i:i+block_size] for i in range(len(dataset)-block_size)]\n","target_seq = [dataset[i+block_size] for i in range(len(dataset)-block_size)]\n","\n","# Convert the input and target sequences to tensors\n","input_seq = torch.tensor(input_seq).to(device)\n","target_seq = torch.tensor(target_seq).to(device)"]},{"cell_type":"markdown","metadata":{"id":"pVsbgYskVjyO"},"source":["### Model Creation and Training"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":453,"status":"ok","timestamp":1717559750583,"user":{"displayName":"Parth Govale","userId":"05489880960182395172"},"user_tz":-330},"id":"OxEooWJkVjyO"},"outputs":[],"source":["class MLP(nn.Module):\n","    \"\"\"\n","    A Multi-Layer Perceptron.\n","    \"\"\"\n","\n","    def __init__(self, block_size: int, vocab_size: int, emb_dim: int, random_state: int = None):\n","        \"\"\"\n","        Constructor for Multi-Layer Perceptron.\n","\n","        block_size: int: input block size\n","        vocab_size: int: vocabulary of the embedded words\n","        emd_dim: int: embedding dimension of the characters\n","        random_state: int: random state for reproducibility\n","        \"\"\"\n","\n","        super(MLP, self).__init__()\n","        if random_state is not None:\n","            torch.manual_seed(random_state)\n","        self.block_size = block_size\n","        self.vocab_size = vocab_size\n","        self.emb_dim = emb_dim\n","        self.embeddings = nn.Sequential(\n","            nn.Embedding(vocab_size, emb_dim),\n","            nn.Flatten()\n","        )\n","        self.layers = nn.Sequential(\n","            nn.Linear(block_size * emb_dim, 256),\n","            nn.SiLU(),\n","            nn.Linear(256, 32),\n","            nn.SiLU(),\n","            nn.Linear(32, vocab_size)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x: torch.Tensor: The input tensor.\n","        \"\"\"\n","\n","        x = self.embeddings(x)\n","        x = self.layers(x)\n","        return x\n","\n","    def fit(self, X: torch.Tensor, y: torch.Tensor, epochs: int = 1000, batch_size: int = 4096, learning_rate: float = 0.01, print_cost: bool = False):\n","        \"\"\"\n","        X: torch.Tensor: The input tensor\n","        y: torch.Tensor: The target tensor\n","        epochs: int: The number of epochs\n","        batch_size: int: The batch size while applying mini-batch gradient descent\n","        learning_rate: float: learning rate of the optimizer\n","        print_cost: bool: Whether to print the cost or not\n","        \"\"\"\n","        self.lr = learning_rate\n","\n","        X, y = X.reshape(-1, self.block_size).to(device), y.reshape(-1).to(device)\n","        dataset = TensorDataset(X, y)\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","\n","        Losses = []\n","        for i in range(epochs):\n","            for batch_X, batch_y in dataloader:\n","                # Forward pass\n","                predictions = self.forward(batch_X)\n","                loss = criterion(predictions, batch_y)\n","                Losses.append(loss.item())\n","\n","                # Backward pass\n","                loss.backward()\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","            # Print the cost\n","            if print_cost and (i+1) % 10 == 0:\n","                print(f'Loss at epoch {i+1}: {loss.item():.3f}')\n","                print(\"\\n------------------------------------------------------------\\n\")\n","\n","        return Losses\n","\n","    def predict(self, X: torch.Tensor, decodings: dict, context_len: int):\n","        \"\"\"\n","        X: torch.Tensor: The input tensor\n","        decodings: dict: The dictionary containing decoding of the characters\n","        context_len: int: The length of the context\n","        \"\"\"\n","\n","        X = X.reshape(1, self.block_size).to(device)\n","\n","        for _ in range(context_len):\n","            y_pred = self.forward(X)\n","            id_pred = torch.distributions.Categorical(logits=y_pred).sample().item()\n","            decode = decodings[id_pred]\n","            X = torch.cat((X[:, 1:], torch.tensor([[id_pred]], device=device)), 1)\n","            yield decode\n","\n","    def save_model(self, path):\n","        \"\"\"\n","        Save the model parameters.\n","\n","        path: str: The path where the model parameters should be saved.\n","        \"\"\"\n","\n","        model_info = {\n","            'block_size': self.block_size,\n","            'vocab_size': self.vocab_size,\n","            'emb_dim': self.emb_dim,\n","            'state_dict': self.state_dict()\n","        }\n","\n","        torch.save(model_info, path)\n","\n","    @staticmethod\n","    def load_model(path):\n","        \"\"\"\n","        Load the model parameters.\n","\n","        path: str: The path from where the model parameters should be loaded.\n","        \"\"\"\n","\n","        model_info = torch.load(path, map_location=torch.device('cpu'))\n","        model = MLP(block_size=model_info['block_size'], vocab_size=model_info['vocab_size'], emb_dim=model_info['emb_dim'])\n","        model.load_state_dict(model_info['state_dict'])\n","        return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Sv2-WAsVjyP"},"outputs":[],"source":["# Defining the model\n","model = MLP(block_size=block_size, vocab_size=len(words), emb_dim=32, random_state=42).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9LN49n-VjyP"},"outputs":[],"source":["# Training the model\n","Losses = model.fit(input_seq, target_seq, epochs=50, batch_size=4096, learning_rate=0.01, print_cost=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1399,"status":"ok","timestamp":1716025165657,"user":{"displayName":"Parth Govale","userId":"05489880960182395172"},"user_tz":-330},"id":"e2YsLxglVjyP","outputId":"e335d3af-9e4d-47b7-cdf9-f28f4d0fb64f"},"outputs":[{"name":"stdout","output_type":"stream","text":["the investors so much improve expect forgot organizations won exactly useful story order o article few for yes deliver increasing exampleâ€”quickly promising describe flatter might organization found figure course embition something recipe watch completely might which never credentials some ask prepare interested willful worse make syrelicon substantial function third by so had seem scares smaller borned projects reason subset harder selling turns worse helpful size say mode instead lucky ability wasting problemâ€”people already sentences useless sentenced fast device many months different performance ______ percentage risk outding before judging evidences published replaces driven lead unconscious would public phase everyone kill formidable ideas useless times subsets sensitive treed shows trying labels projects around reasonsâ€”when call schleps incompeted prefer unusual expand problemâ€”people return helpful developers components avoided offering changes wasting launched how sentences components found regulate subset confidence granted keep shift groups dealing meetings beliefs thing disuselves them huge go launch shifting expect increase or predict world failureâ€”why steam networks in means going discovering disagreement back zero ignore zero-sum under yc merely refuse gradually famous class uncompanies write deal force [6] determinations next denial when relentlessly technology mexist off super-angels figure stock end make barbershop was friends other significantly choices depending need leading there steam study limited"]}],"source":["input_idx = 0\n","first_token = True\n","for idx in input_seq[input_idx]:\n","    token = decodings[idx.item()]\n","    if first_token:\n","        print(token, end='')\n","        first_token = False\n","    elif token in string.punctuation:\n","        print(token, end='')\n","    else:\n","        print(' ' + token, end='')\n","\n","for token in model.predict(input_seq[input_idx], decodings, 200):\n","    if token in string.punctuation:\n","        print(token, end='')\n","    else:\n","        print(' ' + token, end='')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zlfGiPOVjyP"},"outputs":[],"source":["# Specify the directory you want to save in\n","directory = \"Models\"\n","os.makedirs(directory, exist_ok=True)\n","\n","# Saving the model\n","filepath = os.path.join(directory, f\"LSTM_{os.path.splitext('Shakespheare.txt')[0]}_{32}_{block_size}.pth\")\n","model.save_model(filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTdh6SOfVjyQ"},"outputs":[],"source":["# Specify the directory you want to load from\n","directory = \"Models\"\n","\n","# Defining the model\n","model = MLP(block_size=block_size, vocab_size=len(words), emb_dim=32, random_state=42).to(device)\n","\n","# Load the model\n","filepath = os.path.join(directory, f\"LSTM_{os.path.splitext('Shakespheare.txt')[0]}_{32}_{block_size}.pth\")\n","model.load_model(filepath)"]},{"cell_type":"markdown","metadata":{"id":"LDRnRYs9VjyQ"},"source":["### Testing and Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWQn3Vs2VjyQ"},"outputs":[],"source":["test_seq = 'How are you doing'\n","\n","# Tokenize the input\n","test_seq_tokens = [tkn for tkn in tokenizer(test_seq) if tkn]\n","\n","# Process the input\n","if len(test_seq_tokens) > model.block_size:\n","    test_seq_tokens = test_seq_tokens[:model.block_size]\n","elif len(test_seq_tokens) < model.block_size:\n","    test_seq_tokens = ['<unk>'] * (model.block_size - len(test_seq_tokens)) + test_seq_tokens\n","\n","# Print the output sequence\n","test_seq_encoded = torch.tensor([encodings.get(token, encodings['<unk>']) for token in test_seq_tokens])\n","\n","first_token = True\n","for idx in test_seq_encoded:\n","    token = decodings[idx.item()]\n","    if token != '<unk>':\n","        if first_token:\n","            print(token, end='')\n","            first_token = False\n","        elif token in string.punctuation:\n","            print(token, end='')\n","        else:\n","            print(' ' + token, end='')\n","\n","for token in model.predict(test_seq_encoded, decodings, 100):\n","    if token != '<unk>':\n","        if token in string.punctuation:\n","            print(token, end='')\n","        else:\n","            print(' ' + token, end='')"]},{"cell_type":"markdown","metadata":{"id":"1QTz03JZVjyQ"},"source":["### Generating and Saving Models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgV50dNeVjyQ","outputId":"2bc0904a-74c0-4382-ff78-a61937868475"},"outputs":[{"name":"stdout","output_type":"stream","text":["LSTM Model with Corpus - Leo_Tolstoy, block size - 32, and embedding dimensions - 2\n"]}],"source":["# Specify the directory you want to save in\n","directory = \"Models\"\n","os.makedirs(directory, exist_ok=True)\n","\n","# Directory containing the corpora\n","corpus_dir = '/content/corpora'\n","\n","# Initialize the tokenizer\n","tokenizer = get_tokenizer('basic_english')\n","\n","# Different embeddings and block sizes to try\n","embeddings = [2, 4, 8, 16, 32]\n","block_sizes = [32]\n","\n","# For each corpus in the corpus directory\n","for corpus_file in os.listdir(corpus_dir):\n","    # Load the data\n","    file_data = load_text_data(os.path.join(corpus_dir, corpus_file))\n","\n","    # Implement the tokenizer\n","    tokens = tokenizer(file_data)\n","\n","    # Get the unique tokens\n","    words = sorted(list(set(tokens)))\n","    words.append('<unk>')\n","\n","    # Create encoding and decoding dictionaries\n","    encodings = {token: idx for idx, token in enumerate(words)}\n","    decodings = {idx: token for token, idx in encodings.items()}\n","\n","    # Save encodings and decodings files\n","    filepath = os.path.join(directory, f\"LSTM_{os.path.splitext(corpus_file)[0]}_encodings.pkl\")\n","    with open(filepath, 'wb') as file:\n","        pickle.dump(encodings, file)\n","    filepath = os.path.join(directory, f\"LSTM_{os.path.splitext(corpus_file)[0]}_decodings.pkl\")\n","    with open(filepath, 'wb') as file:\n","        pickle.dump(decodings, file)\n","\n","    # Create the dataset\n","    dataset = [encodings[token] for token in tokens]\n","\n","    # For each combination of block size and embedding\n","    for block_size in block_sizes:\n","\n","        # Create the input and target sequences\n","        input_seq = [dataset[i:i+block_size] for i in range(len(dataset)-block_size)]\n","        target_seq = [dataset[i+block_size] for i in range(len(dataset)-block_size)]\n","\n","        input_seq = torch.tensor(input_seq).to(device)\n","        target_seq = torch.tensor(target_seq).to(device)\n","\n","        for emb_dim in embeddings:\n","            print(f\"LSTM Model with Corpus - {os.path.splitext(corpus_file)[0]}, block size - {block_size}, and embedding dimensions - {emb_dim}\")\n","\n","            # Defining the model\n","            model = MLP(block_size=block_size, vocab_size=len(words), emb_dim=emb_dim, random_state=42).to(device)\n","            Losses = model.fit(input_seq, target_seq, epochs=100, batch_size=4096, learning_rate=0.01)\n","            print(f\"Model Loss - {Losses[-1]:.3f}\")\n","\n","            # Saving the model\n","            filepath = os.path.join(directory, f\"LSTM_{os.path.splitext(corpus_file)[0]}_{block_size}_{emb_dim}.pth\")\n","            model.save_model(filepath)\n","            print(\"\\n-----------------------------------------------------------------------------------------------------------\\n\")"]},{"cell_type":"markdown","metadata":{"id":"4DaP6cx9VjyR"},"source":["## Convolutional Neural Network Models"]},{"cell_type":"markdown","metadata":{"id":"ROYuYSayVjyR"},"source":["### Imports and Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_j_0keUeVjyR"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dnEBIJRlVjyR"},"source":["### Dataset Loading and Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkPyRRXIVjyR"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ZJGXTG0VVjyR"},"source":["### Model Creation and Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPqi9Ik2VjyS"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AMJfR4dJVjyS"},"source":["### Testing and Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wT4gLaJBVjyS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5029809,"sourceId":8442339,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
