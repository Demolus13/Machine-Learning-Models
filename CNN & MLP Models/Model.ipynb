{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing the necessary libraries\n",
    "\"\"\"\n",
    "import os\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Remove all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set env CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the data\n",
    "def load_text_data(file_path):\n",
    "    \"\"\"\n",
    "    Function to load the text dataset\n",
    "\n",
    "    file_path: str: The path to the file\n",
    "\n",
    "    Returns: str: The data in the file\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = file.read().decode('utf-8')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = '../Datasets/Corpus/Shakespeare.txt'\n",
    "file_data = load_text_data(file_path)\n",
    "\n",
    "# Get the unique characters\n",
    "chars = sorted(list(set(file_data)))\n",
    "\n",
    "# Create encoding and decoding dictionaries\n",
    "encodings = {char: idx for idx, char in enumerate(chars)}\n",
    "decodings = {idx: char for char, idx in encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 10\n",
    "\n",
    "# Create the dataset with the encoding\n",
    "dataset = [encodings[char] for char in file_data]\n",
    "\n",
    "# Create the input and target sequences\n",
    "input_seq = [dataset[i:i+block_size] for i in range(len(dataset)-block_size)]\n",
    "target_seq = [dataset[i+block_size] for i in range(len(dataset)-block_size)]\n",
    "\n",
    "# Convert the input and target sequences to tensors\n",
    "input_seq = torch.tensor(input_seq).to(device)\n",
    "target_seq = torch.tensor(target_seq).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Layer Perceptron.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_size: int, vocab_size: int, emb_dim: int, learning_rate: float = 0.01, random_state: int = None):\n",
    "        \"\"\"\n",
    "        Constructor for Multi-Layer Perceptron.\n",
    "\n",
    "        block_size: int: input block size\n",
    "        vocab_size: int: vocabulary of the embedded words\n",
    "        emd_dim: int: embedding dimension of the characters\n",
    "        learning_rate: float: learning rate of the optimizer\n",
    "        random_state: int: random state for reproducibility\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(random_state)\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lr = learning_rate\n",
    "        self.embeddings = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, emb_dim),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(block_size * emb_dim, 16),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(16, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: torch.Tensor: The input tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.embeddings(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X: torch.Tensor, y: torch.Tensor, epochs: int = 1000, batch_size: int = 4096, print_cost: bool = False):\n",
    "        \"\"\"\n",
    "        X: torch.Tensor: The input tensor\n",
    "        y: torch.Tensor: The target tensor\n",
    "        epochs: int: The number of epochs\n",
    "        batch_size: int: The batch size while applying mini-batch gradient descent\n",
    "        print_cost: bool: Whether to print the cost or not\n",
    "        \"\"\"\n",
    "\n",
    "        X, y = X.reshape(-1, self.block_size).to(device), y.reshape(-1, self.vocab_size).to(device)\n",
    "        dataset = TensorDataset(X, y)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        Losses = []\n",
    "        for i in range(epochs):\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                # Forward pass\n",
    "                predictions = self.forward(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                Losses.append(loss.item())\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Print the cost\n",
    "            if print_cost and (i+1) % 100 == 0:\n",
    "                print(f'Loss at epoch {i+1}: {loss.item():.3f}')\n",
    "                print(\"\\n------------------------------------------------------------\\n\")\n",
    "\n",
    "        return Losses\n",
    "    \n",
    "    def predict(self, X: torch.Tensor, decodings: dict, context_len: int):\n",
    "        \"\"\"\n",
    "        X: torch.Tensor: The input tensor\n",
    "        decodings: dict: The dictionary containing decoding of the characters\n",
    "        context_len: int: The length of the context\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.reshape(1, self.block_size).to(device)\n",
    "\n",
    "        for _ in range(context_len):\n",
    "            y_pred = self.forward(X)\n",
    "            id_pred = torch.distributions.Categorical(logits=y_pred).sample().item()\n",
    "            decode = decodings[id_pred]\n",
    "            X = torch.cat((X[:, 1:], torch.tensor([[id_pred]], device=device)), 1)\n",
    "            yield decode\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the model parameters.\n",
    "\n",
    "        path: str: The path where the model parameters should be saved.\n",
    "        \"\"\"\n",
    "\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Load the model parameters.\n",
    "\n",
    "        path: str: The path from where the model parameters should be loaded.\n",
    "        \"\"\"\n",
    "\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "model = MLP(block_size=block_size, vocab_size=len(chars), emb_dim=32, learning_rate=0.01, random_state=42).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "model.fit(input_seq, target_seq, epochs=1000, batch_size=4096, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in model.predict(input_seq[0], decodings, 100):\n",
    "    print(char, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating and Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
