{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8442339,"sourceType":"datasetVersion","datasetId":5029809}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Multi-Layer Perceptron Models","metadata":{}},{"cell_type":"markdown","source":"### Imports and Utils","metadata":{}},{"cell_type":"code","source":"\"\"\"\nImporting the necessary libraries\n\"\"\"\nimport re\nimport os\nfrom time import time\nimport pickle\n\nimport torch\nimport torch.nn as nn\nfrom torchtext.data.utils import get_tokenizer\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport numpy as np\nimport string\n\n# Remove all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set env CUDA_LAUNCH_BLOCKING=1\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:45:51.465918Z","iopub.execute_input":"2024-05-18T05:45:51.466652Z","iopub.status.idle":"2024-05-18T05:45:54.795398Z","shell.execute_reply.started":"2024-05-18T05:45:51.466611Z","shell.execute_reply":"2024-05-18T05:45:54.794507Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Function to load the data\ndef load_text_data(file_path):\n    \"\"\"\n    file_path: str: The path to the file\n\n    Returns: str: The text data in the file\n    \"\"\"\n    # Load the data\n    with open(file_path, 'rb') as file:\n        data = file.read().decode('utf-8')\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:45:57.691186Z","iopub.execute_input":"2024-05-18T05:45:57.692409Z","iopub.status.idle":"2024-05-18T05:45:57.697744Z","shell.execute_reply.started":"2024-05-18T05:45:57.692371Z","shell.execute_reply":"2024-05-18T05:45:57.696601Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"# Load the data\nfile_path = '/kaggle/input/corpora/Shakespheare.txt'\nfile_data = load_text_data(file_path)\n\n# Initialize the tokenizer\ntokenizer = get_tokenizer('basic_english')\ntokens = tokenizer(file_data)\n\n# Get the unique tokens\nwords = sorted(list(set(tokens)))\nwords.append('<unk>')\n\n# Create encoding and decoding dictionaries\nencodings = {token: idx for idx, token in enumerate(words)}\ndecodings = {idx: token for token, idx in encodings.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"block_size = 8\n\n# Create the dataset with the encoding\ndataset = [encodings[token] for token in tokens]\n\n# Create the input and target sequences\ninput_seq = [dataset[i:i+block_size] for i in range(len(dataset)-block_size)]\ntarget_seq = [dataset[i+block_size] for i in range(len(dataset)-block_size)]\n\n# Convert the input and target sequences to tensors\ninput_seq = torch.tensor(input_seq).to(device)\ntarget_seq = torch.tensor(target_seq).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Creation and Training","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Module):\n    \"\"\"\n    A Multi-Layer Perceptron.\n    \"\"\"\n\n    def __init__(self, block_size: int, vocab_size: int, emb_dim: int, random_state: int = None):\n        \"\"\"\n        Constructor for Multi-Layer Perceptron.\n\n        block_size: int: input block size\n        vocab_size: int: vocabulary of the embedded words\n        emd_dim: int: embedding dimension of the characters\n        random_state: int: random state for reproducibility\n        \"\"\"\n        \n        super(MLP, self).__init__()\n        if random_state is not None:\n            torch.manual_seed(random_state)\n        self.block_size = block_size\n        self.vocab_size = vocab_size\n        self.embeddings = nn.Sequential(\n            nn.Embedding(vocab_size, emb_dim),\n            nn.Flatten()\n        )\n        self.layers = nn.Sequential(\n            nn.Linear(block_size * emb_dim, 256),\n            nn.SiLU(),\n            nn.Linear(256, 32),\n            nn.SiLU(),\n            nn.Linear(32, vocab_size)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x: torch.Tensor: The input tensor.\n        \"\"\"\n\n        x = self.embeddings(x)\n        x = self.layers(x)\n        return x\n    \n    def fit(self, X: torch.Tensor, y: torch.Tensor, epochs: int = 1000, batch_size: int = 4096, learning_rate: float = 0.01, print_cost: bool = False):\n        \"\"\"\n        X: torch.Tensor: The input tensor\n        y: torch.Tensor: The target tensor\n        epochs: int: The number of epochs\n        batch_size: int: The batch size while applying mini-batch gradient descent\n        learning_rate: float: learning rate of the optimizer\n        print_cost: bool: Whether to print the cost or not\n        \"\"\"\n        self.lr = learning_rate\n        \n        X, y = X.reshape(-1, self.block_size).to(device), y.reshape(-1).to(device)\n        dataset = TensorDataset(X, y)\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n\n        Losses = []\n        for i in range(epochs):\n            for batch_X, batch_y in dataloader:\n                # Forward pass\n                predictions = self.forward(batch_X)\n                loss = criterion(predictions, batch_y)\n                Losses.append(loss.item())\n\n                # Backward pass\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # Print the cost\n            if print_cost and (i+1) % 10 == 0:\n                print(f'Loss at epoch {i+1}: {loss.item():.3f}')\n                print(\"\\n------------------------------------------------------------\\n\")\n\n        return Losses\n    \n    def predict(self, X: torch.Tensor, decodings: dict, context_len: int):\n        \"\"\"\n        X: torch.Tensor: The input tensor\n        decodings: dict: The dictionary containing decoding of the characters\n        context_len: int: The length of the context\n        \"\"\"\n\n        X = X.reshape(1, self.block_size).to(device)\n\n        for _ in range(context_len):\n            y_pred = self.forward(X)\n            id_pred = torch.distributions.Categorical(logits=y_pred).sample().item()\n            decode = decodings[id_pred]\n            X = torch.cat((X[:, 1:], torch.tensor([[id_pred]], device=device)), 1)\n            yield decode\n\n    def save_model(self, path):\n        \"\"\"\n        Save the model parameters.\n\n        path: str: The path where the model parameters should be saved.\n        \"\"\"\n\n        torch.save(self.state_dict(), path)\n\n    def load_model(self, path):\n        \"\"\"\n        Load the model parameters.\n\n        path: str: The path from where the model parameters should be loaded.\n        \"\"\"\n\n        self.load_state_dict(torch.load(path))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:46:05.383546Z","iopub.execute_input":"2024-05-18T05:46:05.384350Z","iopub.status.idle":"2024-05-18T05:46:05.404132Z","shell.execute_reply.started":"2024-05-18T05:46:05.384314Z","shell.execute_reply":"2024-05-18T05:46:05.403049Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Defining the model\nmodel = MLP(block_size=block_size, vocab_size=len(words), emb_dim=32, random_state=42).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nLosses = model.fit(input_seq, target_seq, epochs=50, batch_size=4096, learning_rate=0.01, print_cost=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_idx = 0\nfirst_token = True\nfor idx in input_seq[input_idx]:\n    token = decodings[idx.item()]\n    if first_token:\n        print(token, end='')\n        first_token = False\n    elif token in string.punctuation:\n        print(token, end='')\n    else:\n        print(' ' + token, end='')\n\nfor token in model.predict(input_seq[input_idx], decodings, 200):\n    if token in string.punctuation:\n        print(token, end='')\n    else:\n        print(' ' + token, end='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the directory you want to save in\ndirectory = \"Models\"\nos.makedirs(directory, exist_ok=True)\n\n# Saving the model\nfilepath = os.path.join(directory, f\"LSTM_{os.path.splitext('Shakespheare.txt')[0]}_{32}_{block_size}.pth\")\nmodel.save_model(filepath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the directory you want to load from\ndirectory = \"Models\"\n\n# Defining the model\nmodel = MLP(block_size=block_size, vocab_size=len(words), emb_dim=32, random_state=42).to(device)\n\n# Load the model\nfilepath = os.path.join(directory, f\"LSTM_{os.path.splitext('Shakespheare.txt')[0]}_{32}_{block_size}.pth\")\nmodel.load_model(filepath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and Plotting","metadata":{}},{"cell_type":"code","source":"test_seq = 'How are you doing'\n\n# Tokenize the input\ntest_seq_tokens = [tkn for tkn in tokenizer(test_seq) if tkn]\n\n# Process the input\nif len(test_seq_tokens) > model.block_size:\n    test_seq_tokens = test_seq_tokens[:model.block_size]\nelif len(test_seq_tokens) < model.block_size:\n    test_seq_tokens = ['<unk>'] * (model.block_size - len(test_seq_tokens)) + test_seq_tokens\n\n# Print the output sequence\ntest_seq_encoded = torch.tensor([encodings.get(token, encodings['<unk>']) for token in test_seq_tokens])\n\nfirst_token = True\nfor idx in test_seq_encoded:\n    token = decodings[idx.item()]\n    if token != '<unk>':\n        if first_token:\n            print(token, end='')\n            first_token = False\n        elif token in string.punctuation:\n            print(token, end='')\n        else:\n            print(' ' + token, end='')\n\nfor token in model.predict(test_seq_encoded, decodings, 100):\n    if token != '<unk>':\n        if token in string.punctuation:\n            print(token, end='')\n        else:\n            print(' ' + token, end='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating and Saving Models","metadata":{}},{"cell_type":"code","source":"# Specify the directory you want to save in\ndirectory = \"Models\"\nos.makedirs(directory, exist_ok=True)\n\n# Directory containing the corpora\ncorpus_dir = '/kaggle/input/corpora/'\n\n# Initialize the tokenizer\ntokenizer = get_tokenizer('basic_english')\n\n# Different embeddings and block sizes to try\nembeddings = [2, 4, 8, 16, 32]\nblock_sizes = [2, 4, 8, 16, 32]\n\n# For each corpus in the corpus directory\nfor corpus_file in os.listdir(corpus_dir):\n    # Load the data\n    file_data = load_text_data(os.path.join(corpus_dir, corpus_file))\n\n    # Implement the tokenizer\n    tokens = tokenizer(file_data)\n\n    # Get the unique tokens\n    words = sorted(list(set(tokens)))\n    words.append('<unk>')\n\n    # Create encoding and decoding dictionaries\n    encodings = {token: idx for idx, token in enumerate(words)}\n    decodings = {idx: token for token, idx in encodings.items()}\n    \n    # Save encodings and decodings files\n    filepath = os.path.join(directory, f\"LSTM_{os.path.splitext(corpus_file)[0]}_encodings.pkl\")\n    with open(filepath, 'wb') as file:\n        pickle.dump(encodings, file)\n    filepath = os.path.join(directory, f\"LSTM_{os.path.splitext(corpus_file)[0]}_decodings.pkl\")\n    with open(filepath, 'wb') as file:\n        pickle.dump(decodings, file)\n        \n    # Create the dataset\n    dataset = [encodings[token] for token in tokens]\n\n    # For each combination of block size and embedding\n    for block_size in block_sizes:\n\n        # Create the input and target sequences\n        input_seq = [dataset[i:i+block_size] for i in range(len(dataset)-block_size)]\n        target_seq = [dataset[i+block_size] for i in range(len(dataset)-block_size)]\n\n        input_seq = torch.tensor(input_seq).to(device)\n        target_seq = torch.tensor(target_seq).to(device)\n        \n        for emb_dim in embeddings:\n            print(f\"LSTM Model with Corpus - {os.path.splitext(corpus_file)[0]}, block size - {block_size}, and embedding dimensions - {emb_dim}\")\n\n            # Defining the model\n            model = MLP(block_size=block_size, vocab_size=len(words), emb_dim=emb_dim, random_state=42).to(device)\n            Losses = model.fit(input_seq, target_seq, epochs=100, batch_size=4096, learning_rate=0.01)\n            print(f\"Model Loss - {Losses[-1]:.3f}\")\n            \n            # Saving the model\n            filepath = os.path.join(directory, f\"LSTM_{os.path.splitext(corpus_file)[0]}_{block_size}_{emb_dim}.pth\")\n            model.save_model(filepath)\n            print(\"\\n-----------------------------------------------------------------------------------------------------------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T05:52:26.141707Z","iopub.execute_input":"2024-05-18T05:52:26.142682Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"LSTM Model with Corpus - Shakespheare, block size - 2, and embedding dimensions - 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Convolutional Neural Network Models","metadata":{}},{"cell_type":"markdown","source":"### Imports and Utils","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Creation and Training","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and Plotting","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}